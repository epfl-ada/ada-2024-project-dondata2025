{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184ee2da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:35.031509Z",
     "start_time": "2024-11-14T23:41:34.892663Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e39bb",
   "metadata": {},
   "source": [
    "# Name and movies datasets\n",
    "\n",
    "For this project we decided to work on the CMU movie dataset containing metadata extracted from Freebase, including Movie metadata like Movie box office revenue, genre, release date, runtime, and language but also Character metada like character names and aligned information about the actors who portray them, including gender and estimated age at the time of the movie's release. \n",
    "First let's see what the 2 metadasets contains.\n",
    "\n",
    "#### Characters\n",
    "The dataset contains informations  450,669 characters aligned to the movies such as Wikipedia movie ID,  Freebase movie ID, Movie release date, Character name, Actor date of birth, Actor gender, Actor height (in meters), Actor ethnicity,Actor name, Actor age at movie release, Freebase character/actor map ID, Freebase character ID, Freebase actor ID. \n",
    "\n",
    "\n",
    "#### Movies\n",
    "The dataset contains informations on 81,741 movies such as the Wikipedia movie ID, Freebase movie ID, Movie name, Movie release date, Movie box office revenue, Movie runtime, Movie languages, Movie countries, Movie genres .\n",
    "\n",
    "\n",
    "\n",
    "## Cleaning\n",
    "\n",
    "The cleaning task was implement in the *clean_raw_data()* method of our different CharacterData and MovieData classes implementation (by Wikipedia Movie ID) and validated using the *check_clean_data()* method, available and shared by the 2 datasets (python inheritance).\n",
    "\n",
    "From both metadataset, we directly oberved similar features such as Wikipedia Movie ID and Freebase Movie ID that is useful for futur merge of the 2 dataset. However, as in both datasets we saw that there were several columns containing Freebase and Wikipedia IDs for actors, characters and films, we decided to put them aside as the data is too difficult to access.\n",
    "\n",
    "This are the different steps we applied to both datasets before merging:\n",
    "\n",
    "Character dataset:\n",
    "- Load with the right spacer.\n",
    "- Rename the columns for proper understanding.\n",
    "- Check the good type format : Actor date of birth and the Release Date as a datetime, and the other into objects.\n",
    "- Deal with missing values : we wrote them as NaN or NaT\n",
    "- Droping unwanted columns\n",
    "- Checking that the cleaning was made\n",
    "\n",
    "Movie dataset :\n",
    "- Load with the right spacer.\n",
    "- Rename the columns for proper understanding.\n",
    "- Modify the Language, Country and Genre columns:  the information was a JSON format not readable nor accessible so we isolate the information and replace it by a human-readable string format.\n",
    "- Modify the datatypes of movie runtime into timedelta and the release date into a datetime object for further manipulation.\n",
    "- Modify the movie Name, Language, Country and Genre:  we checked that they were in object type and modified them if not.\n",
    "- Deal with missing values : we wrote them as NaN or NaT\n",
    "- Dropping the unwanted columns\n",
    "- Check that the cleaning was made\n",
    "\n",
    "\n",
    "We kept the whole dataset with NaN and NaT values in a specific file to keep features that could be interesting even if the rate of missing values is very high (such as etchnicity or Box office revenue). However, for the following notebook, to make some we decided to remove this 2 columns since they have more than 70% missing values.\n",
    "\n",
    "\n",
    "## Demo\n",
    "\n",
    "Here, we will import and clean the data base to demonstrate the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb427778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:35.044575Z",
     "start_time": "2024-11-14T23:41:35.037246Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports my code from the file src/data/movies_char_data.py\n",
    "import src.data.movies_char_data as MovieChar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965eb4fa",
   "metadata": {},
   "source": [
    "#### Characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:36.446057Z",
     "start_time": "2024-11-14T23:41:35.216969Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "character_df = MovieChar.CharacterData(\"Character\", \"character.metadata.tsv\", output_name = \"character_data_clean.csv\")\n",
    "character_df.raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abe64c7880d8db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:38.451374Z",
     "start_time": "2024-11-14T23:41:36.543444Z"
    }
   },
   "outputs": [],
   "source": [
    "# print duplicated rows\n",
    "character_df.clean_raw_data()\n",
    "character_df.clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7298820688573702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:42.677542Z",
     "start_time": "2024-11-14T23:41:38.530566Z"
    }
   },
   "outputs": [],
   "source": [
    "character_df.pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb600d2",
   "metadata": {},
   "source": [
    "####  Movie dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949e276",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:43.169758Z",
     "start_time": "2024-11-14T23:41:42.733189Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_df = MovieChar.MovieData(\"Movie\", \"movie.metadata.tsv\", output_name = \"movie_data_clean.csv\")\n",
    "\n",
    "#Display name and file_name\n",
    "print(movie_df.name, movie_df.file_name)\n",
    "\n",
    "movie_df.raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e3120c95d62e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:44.471670Z",
     "start_time": "2024-11-14T23:41:43.245632Z"
    }
   },
   "outputs": [],
   "source": [
    "# print duplicated rows\n",
    "movie_df.clean_raw_data()\n",
    "movie_df.clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8878aae6ad2e2178",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:47.333437Z",
     "start_time": "2024-11-14T23:41:44.558458Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_df.pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6d342e",
   "metadata": {},
   "source": [
    "## Merging Movie and Character into one dataset\n",
    "\n",
    "We merged the 2 dataset following the Wikipedi movie ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c02c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:48.278446Z",
     "start_time": "2024-11-14T23:41:47.424828Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.utils.movies_utils import *\n",
    "\n",
    "mov_char_data = merge_movies_characters_data(movie_df, character_df)\n",
    "\n",
    "mov_char_data.head() # When we call the data name object, it returns the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144abe8c5e2174c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:48.362946Z",
     "start_time": "2024-11-14T23:41:48.359328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print size of the dataset\n",
    "print(mov_char_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206998d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:48.475716Z",
     "start_time": "2024-11-14T23:41:48.460509Z"
    }
   },
   "outputs": [],
   "source": [
    "mov_char_data.head(100) # When we call the data name object, it returns the cleaned data\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e07a9",
   "metadata": {},
   "source": [
    "# Names datasets\n",
    "\n",
    "In order to answer our research questions, we needed to find some birth registries from different countries. Those were freely available and we found datasets for the following countries:\n",
    "\n",
    "- France\n",
    "- USA\n",
    "- United Kingdom\n",
    "- AJOUTER AUTRE SI BESOIN\n",
    "\n",
    "Since they all came from different places and didn't follow the same structure, we had to decide what kind of data was necessary for our project and what structure would be the most practical to work with. We ended-up with the following collumns in our dataframes:\n",
    "\n",
    "1. **Year** : An integer value giving the year of the count \n",
    "2. **Name** :  A string representing the name that was counted\n",
    "3. **Sex** : There are two possible characters, 'F' (female) and 'M' (male)\n",
    "4. **Count** : An integer value giving the count of the name during this year\n",
    "\n",
    "## Data homogenization\n",
    "\n",
    "The cleaning task was implement in the *clean_raw_data()* method of our different NamesData classes implementation (by country) and validated using the *check_clean_data()* method, available and shared by all the name datasets (python inheritance).\n",
    "\n",
    "### Column structure\n",
    "This task was not too difficult since it was mostly reordering, renaming the ones needed and dropping the ones that were not useful for our project. We also made sure that the same type was used on the collumns of the different datasets. \n",
    "\n",
    "### Year \n",
    "All of our dataset had the same year format, but some had missing values in this field, which made those row useless and they were therefore discarded.\n",
    "This collumn made it hard to find datasets from more countries, since a lot of them started to count only in the early 2000's, which doesn't give us enough data to detect real changes in the distribution. (The movie data base ends in 2012)\n",
    "\n",
    "### Name\n",
    "This was the hardest column to sanitize and clean since a lot of variation of a same name are possible. We ended by defining a regex expression do define what we would accept as a valid name : ^[A-Z-\\s\\']+$\n",
    "\n",
    "This allows us to limit ourselves to names composed only of capitalized letters, spaces, '-' for composed names and ''' for the some regional variations. This rule is really strict and would have made us lose a considerable proportion of our dataset. This is where the data cleaning process came to help homogenize our data and it mainly consisted of the following operations:\n",
    "\n",
    "- Converting all the name to uppercase\n",
    "- Removing all accents on letter, for example Ã© becomes e.\n",
    "\n",
    "Some names have different spellings, for example you can write JEREMY and JEREMIE, but we decided to count this as two separate entries since grouping \"similar\" is out of the scope of this project and is not an uniformised concept.\n",
    "\n",
    "### Sex\n",
    "The french dataset had some integer values that we converted to the expected format. This field is useful for our research questions, but complicated the dataset research, since a lot of countries did not include this information in their registries.\n",
    "\n",
    "### Validation\n",
    "\n",
    "The python class representing our datasets contains a method *check_clean_data()* that checks multiple conditions to be sure that the data is uniform. \n",
    "\n",
    "- Checks the collumns' name\n",
    "- Checks if some missing values are present\n",
    "- Checks the data type of each collumn\n",
    "- Checks for duplicated rows (same name, same sex and same name)\n",
    "- Checks that the strings respects the defined regex expressions\n",
    "- Checks that the counts and years are coherent numbers \n",
    "\n",
    "## Demo\n",
    "\n",
    "Here, we will import and clean the data base to demonstrate the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34f05a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:49.166032Z",
     "start_time": "2024-11-14T23:41:48.628928Z"
    }
   },
   "outputs": [],
   "source": [
    "import src.data.names_data as NamesData\n",
    "ukNames = NamesData.UKNamesData(\"UK\", \"ukbabynames.csv\")\n",
    "\n",
    "# The raw data directly from the file\n",
    "ukNames.raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335bd2a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:51.337754Z",
     "start_time": "2024-11-14T23:41:49.241962Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can call the cleaning method, which will correct the columns' names and ordering, and clean the content\n",
    "ukNames.clean_raw_data()\n",
    "ukNames().head() #  This is the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5398679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:53.350866Z",
     "start_time": "2024-11-14T23:41:51.446150Z"
    }
   },
   "outputs": [],
   "source": [
    "frenchNames = NamesData.FranceNamesData(\"France\", \"france.csv\", \"https://www.insee.fr/fr/statistiques/8205621?sommaire=8205628#dictionnaire\", \";\")\n",
    "\n",
    "frenchNames.raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e43bd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:58.004962Z",
     "start_time": "2024-11-14T23:41:53.435250Z"
    }
   },
   "outputs": [],
   "source": [
    "frenchNames.clean_raw_data()\n",
    "frenchNames().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31df7ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:41:59.162959Z",
     "start_time": "2024-11-14T23:41:58.125143Z"
    }
   },
   "outputs": [],
   "source": [
    "USNames = NamesData.USNamesData(\"US\", \"babyNamesUSYOB-full.csv\")\n",
    "USNames.raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa5b62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:42:03.493624Z",
     "start_time": "2024-11-14T23:41:59.245885Z"
    }
   },
   "outputs": [],
   "source": [
    "USNames.clean_raw_data()\n",
    "USNames().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76753e",
   "metadata": {},
   "source": [
    "### Merging the datasets\n",
    "If we want to answer a question with no regards to the provenance of the names, we can use our function to group all the datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb777dfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:42:17.302933Z",
     "start_time": "2024-11-14T23:42:03.618736Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.utils.names_utils import merge_names_data\n",
    "\n",
    "global_names = merge_names_data([ukNames, frenchNames, USNames])\n",
    "global_names().head()\n",
    "print(f\"The merged dataset contains {global_names().shape[0]} rows ! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff56f384",
   "metadata": {},
   "source": [
    "## Feature Visualization\n",
    "\n",
    "Lets visualize the different information from the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d5627dff4aa0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:42:17.386027Z",
     "start_time": "2024-11-14T23:42:17.377035Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.utils.data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb9eaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:42:17.606378Z",
     "start_time": "2024-11-14T23:42:17.524810Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of Nan values in the Movies & Character dataset\n",
    "mov_char_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f478355d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T23:42:18.412218Z",
     "start_time": "2024-11-14T23:42:17.871853Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizing the number of missing values per columns\n",
    "nan_percentage = mov_char_data.isnull().mean().sort_values(ascending=False)\n",
    "\n",
    "# Plot the percentage of NaN values per column\n",
    "ax = nan_percentage.plot(kind='bar', figsize=(16, 8), color='skyblue')\n",
    "plt.ylabel('Percentage of NaN values')\n",
    "plt.title('Percentage of NaN values in % per column', fontsize=20)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.2%}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703a894",
   "metadata": {},
   "source": [
    "This graph illustrates the distribution of columns based on the percentage of NaN values present in each. It helps us understand how missing data is spread across different features, highlighting columns with higher or lower levels of incompleteness\n",
    "\n",
    "Top 50 movies revenue \n",
    "This graph displays the top 50 movies based only on the revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f6458c",
   "metadata": {},
   "source": [
    "## Trend evaluation\n",
    "To assess what impact a movie had on child naming, we first try a simple model that computes the average count of babies named like a character 5 years before and 5 years after the movie's release, and compute their difference. The higher it is, the higher the trend the year the film was released.\n",
    "\n",
    "Here is a demo of the model printing the top10 trend-inducing character names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining if a movie is a blockbuster with IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff6284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.trend_evaluation import trend_eval_ranking\n",
    "from src.models.imdb_manipulation import get_movie_votes, merge_with_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "932e3949",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Get the IMDB votes for the movies (Warning : might take a few minutes, nearly 700K rows !)\n",
    "imdb_votes = get_movie_votes(\"data/raw/imdb\")\n",
    "imdb_votes.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a92e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_votes.sort_values(by=\"averageRating\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate for movie because there are votings from different countries so we need to aggregate the data\n",
    "\n",
    "aggregated_imdb = imdb_titles.groupby('primaryTitle').apply(\n",
    "    lambda group: pd.Series({\n",
    "        'weightedAverageRating': (group['numVotes'] * group['averageRating']).sum() / group['numVotes'].sum(),\n",
    "        'totalVotes': group['numVotes'].sum()\n",
    "    })\n",
    ").reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44711ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the result\n",
    "aggregated_imdb.sort_values('weightedAverageRating', ascending=False).head(10)\n",
    "aggregated_imdb.sort_values('totalVotes', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6420162",
   "metadata": {},
   "source": [
    "### Merging the IMDB dataset with mov_char_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082dcc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the IMDB votes and rating with the dataset of movie names\n",
    "merge_with_rating = merge_with_characters(aggregated_imdb, mov_char_data)\n",
    "merge_with_rating.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add760c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3369ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new dataframe from mov_char_data with only the Movie_name\n",
    "movie_name_df = mov_char_data[[\"Movie_name\",\"Wikipedia_movie_ID\"]]\n",
    "print(movie_name_df.shape)\n",
    "movie_name_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7efb79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate from the movie_name_df\n",
    "movie_name_df = movie_name_df.drop_duplicates()\n",
    "print(movie_name_df.shape)\n",
    "movie_name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f30416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the IMDB votes and rating with the dataset of movie names\n",
    "merge_with_rating = merge_with_characters(imdb_titles, movie_name_df)\n",
    "print(merge_with_rating.shape)\n",
    "merge_with_rating.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19d7791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate for movie because there are votings from different countries so we need to aggregate the data\n",
    "\n",
    "aggregated_data = merge_with_rating.groupby('primaryTitle').apply(\n",
    "    lambda group: pd.Series({\n",
    "        'weightedAverageRating': (group['numVotes'] * group['averageRating']).sum() / group['numVotes'].sum(),\n",
    "        'totalVotes': group['numVotes'].sum()\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Display the result\n",
    "aggregated_data.sort_values('weightedAverageRating', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data.sort_values('totalVotes', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1179c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_blockbuster(row, votes_threshold=1000000, rating_threshold=8.0):\n",
    "    \"\"\"\n",
    "    Determines if a movie is a blockbuster based on total votes and weighted average rating.\n",
    "\n",
    "    :param row: A row of the DataFrame\n",
    "    :param votes_threshold: The minimum number of votes to qualify as a blockbuster\n",
    "    :param rating_threshold: The minimum average rating to qualify as a blockbuster\n",
    "    :return: Boolean (True if blockbuster, False otherwise)\n",
    "    \"\"\"\n",
    "    return row['totalVotes'] > votes_threshold and row['weightedAverageRating'] >= rating_threshold\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "aggregated_data['isBlockbuster'] = aggregated_data.apply(is_blockbuster, axis=1)\n",
    "\n",
    "# Display the first few rows\n",
    "display(aggregated_data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f101b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for blockbusters\n",
    "blockbusters = aggregated_data[aggregated_data['isBlockbuster']]\n",
    "blockbusters.sort_values('weightedAverageRating', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83953bd0",
   "metadata": {},
   "source": [
    "### Emile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ddbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.trend_evaluation import trend_eval_ranking\n",
    "from src.models.imdb_manipulation import get_movie_votes, merge_with_characters\n",
    "\n",
    "# Get the IMDB votes for the movies (Warning : might take a few minutes, nearly 700K rows !)\n",
    "imdb_titles = get_movie_votes(\"data/raw/imdb\")\n",
    "# Merge the movies and characters data with the IMDB votes\n",
    "char_rating = merge_with_characters(imdb_titles, mov_char_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_characters(imdb_df, characters_df):\n",
    "    \"\"\"\n",
    "    Function to merge the IMDb data with the characters data.\n",
    "    Ensures no duplicate rows are added and only matches with `Movie_name` in `characters_df` are considered.\n",
    "    :param imdb_df: DataFrame\n",
    "    :param characters_df: DataFrame\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    # Merge based on primaryTitle\n",
    "    char_rating = characters_df.merge(\n",
    "        imdb_df[['primaryTitle', 'averageRating', 'numVotes']], \n",
    "        left_on='Movie_name', \n",
    "        right_on='primaryTitle', \n",
    "        how='left'\n",
    "    )\n",
    "    # Drop the redundant 'primaryTitle' column\n",
    "    char_rating = char_rating.drop(columns=['primaryTitle'])\n",
    "    \n",
    "    # Merge based on originalTitle to fill missing data\n",
    "    char_rating = char_rating.merge(\n",
    "        imdb_df[['originalTitle', 'averageRating', 'numVotes']],\n",
    "        left_on='Movie_name',\n",
    "        right_on='originalTitle',\n",
    "        how='left',\n",
    "        suffixes=('_primary', '_original')\n",
    "    )\n",
    "    # Use primary title data if available, otherwise fallback to original title\n",
    "    char_rating['averageRating'] = char_rating['averageRating_primary'].combine_first(char_rating['averageRating_original'])\n",
    "    char_rating['numVotes'] = char_rating['numVotes_primary'].combine_first(char_rating['numVotes_original'])\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    char_rating = char_rating.drop(columns=['originalTitle', 'averageRating_primary', 'averageRating_original', 'numVotes_primary', 'numVotes_original'])\n",
    "\n",
    "    print(f\"There are {char_rating.shape[0]} rows in the merged dataset after ensuring no duplicates are added.\")\n",
    "    return char_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e151850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234805e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# studing the number of nan in the rating column\n",
    "char_rating.isna().sum() \n",
    "\n",
    "# Printing and visualizing the year of which the movies that have Nan value in the averageRating colum are released\n",
    "nan_rating = char_rating[char_rating['averageRating'].isna()]\n",
    "nan_rating['Release_date'] = nan_rating['Release_date'].apply(lambda x: x.year)\n",
    "nan_rating['Release_date'].value_counts().sort_index().plot(kind='bar', figsize=(16, 8), color='skyblue')\n",
    "plt.ylabel('Number of movies with NaN averageRating')\n",
    "plt.xlabel('Release_date')\n",
    "plt.title('Number of movies with NaN averageRating per year', fontsize=20)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c96cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset with the movies that have a rating\n",
    "char_rating_cleaned = char_rating.dropna(subset=['averageRating'])\n",
    "\n",
    "#what is the size of the dataset\n",
    "print(char_rating_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d745dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking top 10 influencing character names by trend increase\n",
    "ranking = trend_eval_ranking(global_names.clean_df, char_rating)\n",
    "print(ranking[[\"Character_name\",\"movie_name\",\"release_year\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8594d30",
   "metadata": {},
   "source": [
    "### Trend visualisation\n",
    "Using the previously computed trend-inducing movies, we can now plot the baby name popularity over time with a red line on the year of the most influential movie for this name.\n",
    "\n",
    "Note that we need to indicate the name in uppercase for compatibility with name datasets and add the gender M/F to avoid confusion for androgenous names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d189a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.trend_evaluation import plot_trend\n",
    "\n",
    "plot_trend(\"NEO\", \"M\", ranking, global_names.clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e102211",
   "metadata": {},
   "source": [
    "## Name prediction\n",
    "\n",
    "To answer our research questions, we needed to find a method to determine if after a specific date, the count of name would follow an abnormal evolution.\n",
    "\n",
    "There are multiple ways to do it and for our first tentative, we decided to try interrupted time series.\n",
    "\n",
    "### ITS - Interrupted time series\n",
    "The concept is rather simple: at a specific point in time, we split our measurements in two parts and use the first one to train a model. This model will try to predict what the evolution would have been based on the previous behaviour and once we get it, we can compare it with the second part of the data that we kept. \n",
    "\n",
    "As mentioned in the explanation, we need to chose a model for this and after some researches, we decided to try the two following ones.\n",
    "\n",
    "We are still evaluating how well they are suited for our project, since the training sample is quite limited due to the granularity of the data. (Count is by year)\n",
    "\n",
    "#### SARIMA - Seasonal Autoregressive Integrated Moving Average\n",
    "Well known model for univariate time series forecasting, SARIMA is an extension of the ARIMA model and adds the support for time series with a seasonal behaviour in addition to the trend support of ARIMA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec84451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.naming_prediction import predict_naming_ARIMA\n",
    "\n",
    "prediction = predict_naming_ARIMA(global_names, \"LUKE\", 1976, 10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f47ba8e",
   "metadata": {},
   "source": [
    "Here we asked our SARIMA model to forecast the counts for the name \"Luke\" from the year after the year 1976, which is when the first Star Wars movie was released. \n",
    "\n",
    "We can see that the the modelled curve has a slower growth than the actual one and this can be used to show an abnormal evolution of the count."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02c2ec",
   "metadata": {},
   "source": [
    "#### Prophet\n",
    "Developped by Facebook, Prophet is a fully automatic procedure made for time series forecasting that is used in various context due it's wide range of features (seasonality, holidy effect, ...)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.naming_prediction import predict_naming_prophet\n",
    "\n",
    "prediction = predict_naming_prophet(global_names, \"LUKE\", 1976, 10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059c35b",
   "metadata": {},
   "source": [
    "This time, we use Facebook's Prophet to forecast the counts for the same parameters and we can already see a difference between the two models. Prophet is generally more resistant to outliers and here, this leads to a more important difference between the modelled data and the actual one.\n",
    "\n",
    "For now, those are only observations and we'll be investigated more thoroughly in the following days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f6ad5",
   "metadata": {},
   "source": [
    "#### Model conclusion\n",
    "We still need to compare the two models and see if the ITS approach would be beneficial for our project since other options are available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
